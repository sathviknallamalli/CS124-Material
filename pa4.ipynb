{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CS 124 Programming Assignment 4: Information Retrieval (Winter 2025)\n",
    "\n",
    "In this assignment you will be improving upon a rather poorly-made information retrieval system. You will build an inverted index to quickly retrieve documents that match queries and then make it even better by using term-frequency inverse-document-frequency weighting and cosine similarity to compare queries to your data set. Your IR system will be evaluated for accuracy on the correct documents retrieved for different queries and the correctly computed tf-idf values and cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "You will be using your IR system to find relevant documents among a collection of sixty books sourced from Project Gutenberg. The training data is located in the data/ directory under the subdirectory ProjectGutenberg/. Within this directory you will see yet another directory raw/. This contains the raw text files of the sixty short stories. The data/ directory also contains the files dev_queries.txt and dev_solutions.txt. We have provided these to you as a set of development queries and their expected answers to use as you begin implementing your IR system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Improve upon the given IR system by implementing the following features:\n",
    "\n",
    "<b>Inverted Positional Index:</b> Implement an inverted index - a mapping from words to the documents in which they occur, as well as the positions in the documents for which they occur.\n",
    "\n",
    "<b>Boolean Retrieval:</b> Implement a Boolean retrieval system, in which you return the list of documents that contain all words in a query. (Yes, you only need to support conjunctions for this assignment.)\n",
    "\n",
    "<b>Phrase Query Retrieval:</b> Implement a system that returns the list of documents in which the full phrase appears, (ie. the words of the query appear next to each other, in the specified order). Note that at the time of retrieval, you will not have access to the original documents anymore (the documents would be turned into bag of words), so you'll have to utilize your inverted positional index to complete this part.\n",
    "\n",
    "<b>TF-IDF:</b> Compute and store the term-frequency inverse-document-frequency value for every word-document co-occurrence.\n",
    "\n",
    "<b>Cosine Similarity:</b> Implement cosine similarity in order to improve upon the ranked retrieval system, which currently retrieves documents based on the Jaccard coefficient between the query and each document. The reference solution uses <b> log tf-idf weighting </b> from the lecture for computing cosine scores. That is, $\\cos(\\theta) = \\frac{\\sum_{t \\in Q \\cap D} w_{t,d} \\cdot w_{t,q}}{\\sqrt{\\sum_{t} w_{t,d}^2} \\cdot \\sqrt{\\sum_{t} w_{t,q}^2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your IR system will be evaluated on a small development set of queries as well as a held-out set of queries. The development queries are encoded in the file <b>dev_queries.txt</b> and are:\n",
    "\n",
    "- chest of drawers\n",
    "- machine learning is cool\n",
    "- the cold breeze\n",
    "- pacific coast\n",
    "- pumpkin pies\n",
    "- i completed fizzbuzz\n",
    "\n",
    "We test your system based on the five parts mentioned above: the inverted index (used both to get word positions and to get postings), boolean retrieval, phrase query retrieval, computing the correct tf-idf values, and implementing cosine similarity using the tf-idf values. The provided development queries contain some common words, some uncommon words, and the occasional non-existent word. Some of the query phrases are found verbatim in the book dataset, and some are not. Your system should be able to correctly handle all of these cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Before we do anything else, let's quickly check that you're running the correct\n",
    "version of Python and are in the right environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs124\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell complains, it means that you're using the wrong environment\n",
    "or Python version!\n",
    "\n",
    "If so, please exit this notebook, kill the notebook server with CTRL-C, and\n",
    "try running\n",
    "\n",
    "$ conda activate cs124\n",
    "\n",
    "then restarting your notebook server with\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "If that doesn't work, you should go back and follow the installation\n",
    "instructions in PA0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "We will first start by importing some modules and setting up our IR system class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from porter_stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile('(.*) \\d+\\.txt')\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are not 60 documents in ../data/ProjectGutenberg/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/ProjectGutenberg/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index (data structure that keeps track of the documents in which a particular word is contained, and the positions of that word in the document). The documents will have already been read in at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: titles (a list of strings), docs (a list of lists of strings), and vocab (a list of strings). Since the majority of your work in this assignment will use document ID, we recommend doing the mapping using the document IDs (i.e. the index of the document within `self.docs`). You can then retrieve the text and title of any given document by indexing into `self.docs` and `self.titles` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Build an index of the documents.\n",
    "        \"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Create an inverted, positional index.\n",
    "        #       Granted this may not be a linked list as in a proper\n",
    "        #       implementation.\n",
    "        #       This index should allow easy access to both \n",
    "        #       1) the documents in which a particular word is contained, and \n",
    "        #       2) for every document, the positions of that word in the document \n",
    "        #       Some helpful instance variables:\n",
    "        #         * self.docs = List of documents\n",
    "        #         * self.titles = List of titles\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # Generate inverted index here\n",
    "        for i in range(len(self.docs)):\n",
    "            docwords = self.docs[i]\n",
    "            for j in range(len(docwords)):\n",
    "                word = docwords[j]\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = [[] for _ in range(len(self.docs))]\n",
    "                    inv_index[word][i].append(j)\n",
    "                else:\n",
    "                    inv_index[word][i].append(j)\n",
    "                \n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # turn self.docs into a map from ID to bag of words\n",
    "        id_to_bag_of_words = {}\n",
    "        for d, doc in enumerate(self.docs):\n",
    "            bag_of_words = set(doc)\n",
    "            id_to_bag_of_words[d] = bag_of_words\n",
    "        self.docs = id_to_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement `get_word_positions`. This method returns a list of integers that identifies the positions in the document `doc` (represented as document ID) in which the word is found.  This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully. \n",
    "\n",
    "**Be careful when accessing your inverted index!** Trying to index into a dictionary using a key that is not present will cause an error (and may cause your submission to crash the autograder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_word_positions(self, word, doc):\n",
    "        \"\"\"\n",
    "        Given a word and a document, use the inverted index to return\n",
    "        the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of positions for a word in a document.\n",
    "        positions = []\n",
    "        if word not in self.inv_index:\n",
    "            return []\n",
    "        positions = self.inv_index[word][doc]\n",
    "\n",
    "        return positions\n",
    "        # ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add another method, `get_posting`, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully.\n",
    "\n",
    "Keep in mind that the document IDs in each postings list to be sorted in order to perform the linear merge for boolean retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of postings for a word.\n",
    "        posting = []\n",
    "        if word not in self.inv_index:\n",
    "            return []\n",
    "        occurences = self.inv_index[word]\n",
    "        for i in range(len(occurences)):\n",
    "            if len(occurences[i]) > 0:\n",
    "                posting.append(i)\n",
    "        posting.sort()\n",
    "\n",
    "        return posting\n",
    "        # ------------------------------------------------------------------\n",
    "    \n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_posting(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in query occur.\n",
    "\n",
    "You're welcome to implement the linear merge algorithm outlined in the videos/book or any other method you prefer, but do not use built-in set intersection functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Boolean retrieval. You will want to use your\n",
    "        #       inverted index that you created in index().\n",
    "        # Right now this just returns all the possible documents!\n",
    "        docs = []\n",
    "        words = query\n",
    "        for i in range(len(self.docs)):\n",
    "            occurrences_of_each_word_in_query = []\n",
    "            #check if this doc has all the words\n",
    "            for word in words:\n",
    "                occurrences = self.get_word_positions(word, i)\n",
    "                occurrences_of_each_word_in_query.append(len(occurrences))\n",
    "            \n",
    "            #if number of occurences for each word is greater than 0, then can add\n",
    "            all_exist = True\n",
    "            for occ in occurrences_of_each_word_in_query:\n",
    "                if occ <= 0:\n",
    "                    all_exist = False\n",
    "                    break\n",
    "            \n",
    "            if all_exist:\n",
    "                docs.append(i)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to phase query retrieval. Our `phrase_retrieve` method will return a list of document IDs corresponding to the documents in which all the actual query phrase occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def phrase_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of an ordered list of *stemmed* words, this \n",
    "        returns the list of documents in which *all* of those words occur, and \n",
    "        in the specified order. \n",
    "        Return an empty list if the query does not return any documents. \n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Phrase Query retrieval (ie. return the documents \n",
    "        #       that don't just contain the words, but contain them in the \n",
    "        #       correct order) You will want to use the inverted index \n",
    "        #       that you created in index(), and may also consider using\n",
    "        #       boolean_retrieve. \n",
    "        #       NOTE that you no longer have access to the original documents\n",
    "        #       in self.docs because it is now a map from doc IDs to set\n",
    "        #       of unique words in the original document.\n",
    "        # Right now this just returns all possible documents!\n",
    "        docs = []\n",
    "        querywords = query\n",
    "        \n",
    "        #all words must occur in the doc first\n",
    "        docs_with_the_words = self.boolean_retrieve(query)\n",
    "        \n",
    "        def helpfunc(index_lists, words):\n",
    "            for start_idx in index_lists[0]:\n",
    "                if all(start_idx + i in index_lists[i] for i in range(1, len(words))):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        for d in docs_with_the_words:\n",
    "            #get the positions of the words \n",
    "            index_lists = [self.inv_index[word][d] for word in querywords]\n",
    "            dochasit = helpfunc(index_lists, querywords)\n",
    "            if dochasit:\n",
    "                docs.append(d)\n",
    "            \n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and score the tf-idf values. compute_tfidf and stores the tf-idf values for words and documents. For this you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement `get_tfidf` to return the tf-idf weight for a particular word and document ID. Make sure you correctly handle the case where the word isn't present your vocabulary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def compute_tfidf(self):\n",
    "        # -------------------------------------------------------------------\n",
    "        # TODO: Compute and store TF-IDF values for words and documents in self.tfidf.\n",
    "        #       Recall that you can make use of:\n",
    "        #         * self.vocab: a list of all distinct (stemmed) words\n",
    "        #         * self.docs: a dictionary mapping document ID to a \n",
    "        #                       the set of words that appear in the doc\n",
    "        #         * self.get_word_positions(word, doc): returns list of \n",
    "        #                       positions (i.e. all occurrences) of the given\n",
    "        #                       word within the given document \n",
    "        #         * self.get_posting(word): returns the list of document IDs\n",
    "        #                       for docs containing the given word\n",
    "        #       NOTE that you probably do *not* want to store a value for every\n",
    "        #       word-document pair, but rather just for those pairs where a\n",
    "        #       word actually occurs in the document.\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "        self.idf = {}\n",
    "        \n",
    "        tf_map = {}\n",
    "        idf_map = {}\n",
    "        n = -1\n",
    "        for word in self.inv_index:\n",
    "            docs = self.inv_index[word]\n",
    "            n = len(docs)\n",
    "            tfvals = []\n",
    "            \n",
    "            number_of_documents_in_which_term_occurs = 0\n",
    "            for d in docs:\n",
    "                if len(d) == 0:\n",
    "                    tfvals.append(0)\n",
    "                else:\n",
    "                    tfvals.append(1 + math.log10(len(d)))\n",
    "                    number_of_documents_in_which_term_occurs += 1\n",
    "            tf_map[word] = tfvals\n",
    "            idf_map[word] = math.log10(n / number_of_documents_in_which_term_occurs)\n",
    "            \n",
    "        tfidf_map = {}\n",
    "        for word in tf_map:\n",
    "            tfidflist = [x * idf_map[word] for x in tf_map[word]]\n",
    "            tfidf_map[word] = tfidflist\n",
    "            \n",
    "        self.tfidf = tfidf_map\n",
    "        self.idf = idf_map\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Return the tf-idf weigthing for the given word (string) and\n",
    "        #       document index.\n",
    "        tfidf = 0.0\n",
    "        if word not in self.tfidf:\n",
    "            return 0\n",
    "        tfidf = self.tfidf[word][document]\n",
    "        # ------------------------------------------------------------------\n",
    "        return tfidf\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        This function gets the TF-IDF of an *unstemmed* word in a document.\n",
    "        Stems the word and then calls get_tfidf. You should *not* need to\n",
    "        change this interface, but it is necessary for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_tfidf(word, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will implement `rank_retrieve`. This function returns sorted list of the top ranked documents for a given query. Right now it ranks documents according to their Jaccard similarity with the query, but you will replace this method of ranking with a ranking using the <b>cosine similarity</b> between the documents and query.\n",
    "    \n",
    "Remember to use log TF-IDF weighting for both the query and the document when computing cosine similarity. This means that the query term weights will be computed as: $w_{t,q} = (1 + \\log_{10} \\operatorname{count}(t, q)) \\cdot \\operatorname{IDF}(t)$, where IDF (inverse document frequency) is: $\\operatorname{IDF}(t) = \\log_{10} \\left( \\frac{N}{df_t} \\right)$, where $N$ is the total number of documents and $df_t$ is the number of documents containing term $t$. The document term weights will be: $w_{t,d} = \\operatorname{TF-IDF}(t, d)$, where TF-IDF for the document is: $\\operatorname{TF-IDF}(t, d) = (1 + \\log_{10} \\operatorname{count}(t, d)) \\cdot \\operatorname{IDF}(t)$. Finally, cosine similarity is computed as the dot product of the document and query vectors, normalized by their magnitudes: $\\cos(\\theta) = \\frac{\\sum_{t \\in Q \\cap D} w_{t,d} \\cdot w_{t,q}}{\\sqrt{\\sum_{t} w_{t,d}^2} \\cdot \\sqrt{\\sum_{t} w_{t,q}^2}}$. Refer to the lecture for a more detailed explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "#         scores = [0.0 for xx in range(len(self.titles))]\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement cosine similarity between a document and a list of\n",
    "        #       query words.\n",
    "\n",
    "        # Right now, this code is a placeholder that simply gets the score by\n",
    "        # taking the Jaccard similarity between the query and every document. \n",
    "        # Please replace with your implementation of cosine similarity-based retrieval.\n",
    "        query_words = query\n",
    "        querymap = {}\n",
    "        for qw in query_words:\n",
    "            if qw not in querymap:\n",
    "                querymap[qw] = 1\n",
    "            else:\n",
    "                querymap[qw] += 1\n",
    "                \n",
    "        tfidf_query = {}\n",
    "        q_denom = 0\n",
    "        for qw in querymap:\n",
    "            tf = 1 + math.log10(querymap[qw])\n",
    "            if qw not in self.idf:\n",
    "                idf = 0\n",
    "            else:\n",
    "                idf = self.idf[qw]\n",
    "            tfidf_query[qw] = tf * idf\n",
    "            q_denom += (tfidf_query[qw])**2\n",
    "        q_denom = math.sqrt(q_denom)\n",
    "            \n",
    "        scores = {}\n",
    "        for i in range(len(self.titles)):\n",
    "            \n",
    "            unique_words_in_doc = self.docs[i]\n",
    "            d_denom = 0\n",
    "            for w in unique_words_in_doc:\n",
    "                d_denom += self.tfidf[w][i] ** 2\n",
    "            d_denom = math.sqrt(d_denom)\n",
    "            \n",
    "            curscore = 0\n",
    "            for qw in query_words:\n",
    "                num1 = tfidf_query[qw]\n",
    "                if qw not in self.tfidf:\n",
    "                    num2 = 0\n",
    "                else:\n",
    "                    num2 = self.tfidf[qw][i]\n",
    "                curscore += ((num1 / q_denom) * (num2 / d_denom))\n",
    "            scores[i] = curscore\n",
    "                \n",
    "        sorted_keys = [k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "        rankings = []\n",
    "        for i in sorted_keys:\n",
    "            rankings.append((i, scores[i]))\n",
    "        return rankings\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem): \n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the `run_tests` and `run_query` functions to test your code. `run_tests` tests how different components your search engine code perform on a small set of queries and checks whether or not it matches up with our solution's results. `run_query` can be used to test your code on individual queries. \n",
    "\n",
    "Note that the first run for either of these functions might take a little while, since it will stem all the words in every document create a directory named stemmed/ in ../data/ProjectGutenberg/. This is meant to be a simple cache for the stemmed versions of the text documents. Later runs will be much faster after the first run since all the stemming will already be completed. However, this means that **if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in ../data/ProjectGutenberg/stemmed/. If this happens, simply remove the stemmed/ directory and re-run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(6):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "        \n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # phrase query test\n",
    "            print(\"Phrase Query Retrieval\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.phrase_query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and \\\n",
    "                        guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 5:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and \\\n",
    "                            top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % \\\n",
    "                   (num_correct, num_total, float(num_correct) / num_total)\n",
    "\n",
    "        if part == 1:\n",
    "            if num_correct == num_total:\n",
    "                points = 2\n",
    "            elif num_correct >= 0.5 * num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        elif part == 2:\n",
    "            if num_correct == num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        else:\n",
    "            if num_correct == num_total:\n",
    "                points = 3\n",
    "            elif num_correct > 0.75 * num_total:\n",
    "                points = 2\n",
    "            elif num_correct > 0:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "\n",
    "        print(\"    Score: %d Feedback: %s\" % (points, feedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "===== Running tests =====\n",
      "Inverted Index Test\n",
      "    Score: 3 Feedback: 6/6 Correct. Accuracy: 1.000000\n",
      "Get Postings Test\n",
      "    Score: 2 Feedback: 6/6 Correct. Accuracy: 1.000000\n",
      "Boolean Retrieval Test\n",
      "    Score: 1 Feedback: 6/6 Correct. Accuracy: 1.000000\n",
      "Phrase Query Retrieval\n",
      "    Score: 3 Feedback: 6/6 Correct. Accuracy: 1.000000\n",
      "TF-IDF Test\n",
      "    Score: 3 Feedback: 6/6 Correct. Accuracy: 1.000000\n",
      "Cosine Similarity Test\n",
      "    Score: 3 Feedback: 6/6 Correct. Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to run the tests\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/ProjectGutenberg')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "run_tests(irsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'My very own query':\n",
      "Anna Karenina - Leo Tolstoy: 1.382642e-02\n",
      "The Scarlet Letter - Nathaniel Hawthorne: 1.199004e-02\n",
      "Around the World in Eighty Days - Jules Verne: 1.155807e-02\n",
      "Dracula - Bram Stoker: 1.010096e-02\n",
      "Violin Mastery_Talks with Master Violinists and Teachers - Frederick Herman Martens: 9.957106e-03\n",
      "The Iliad - Homer: 8.318973e-03\n",
      "War and Peace - Leo Tolstoy: 5.910470e-03\n",
      "Don Quixote - Miguel de Cervantes Saavedra: 5.333663e-03\n",
      "Our Vanishing Wild Life_Its Extermination and Preservation - William T Hornaday: 5.221044e-03\n",
      "Les Misérables - Victor Hugo: 5.050607e-03\n",
      "The Count of Monte Cristo - Alexandre Dumas and Auguste Maquet: 4.987717e-03\n",
      "Anthem - Ayn Rand: 1.705823e-04\n",
      "The Sorrows of Young Werther - Johann Wolfgang von Goethe: 1.108843e-04\n",
      "The Fables of Aesop - Aesop: 1.040306e-04\n",
      "Frankenstein or the Modern Prometheus - Mary Wollstonecraft Shelley: 1.037547e-04\n",
      "The Time Machine - H G Wells: 1.001523e-04\n",
      "Metamorphosis - Franz Kafka: 9.882759e-05\n",
      "Life Is a Dream - Pedro Calderón de la Barca: 8.398779e-05\n",
      "Macbeth - William Shakespeare: 8.271422e-05\n",
      "Autobiography - John Stuart Mill: 7.797314e-05\n",
      "The Oedipus Trilogy - Sophocles: 7.790395e-05\n",
      "Hippolytus and the Bacchae - Euripides: 7.559407e-05\n",
      "The Training of a Forester - Gifford Pinchot: 7.455985e-05\n",
      "Pride and Prejudice - Jane Austen: 7.407127e-05\n",
      "Antarctic Penguins-A Study of Their Social Habits - G Murray Levick: 7.325443e-05\n",
      "The Chemical History of a Candle - Michael Faraday: 7.150834e-05\n",
      "A Journey to the Centre of the Earth - Jules Verne: 7.041277e-05\n",
      "The Phantom of the Opera - Gaston Leroux: 6.913725e-05\n",
      "The War of the Worlds - H G Wells: 6.712637e-05\n",
      "The Adventures of Sherlock Holmes - Arthur Conan Doyle: 6.622341e-05\n",
      "Great Pianists on Piano Playing - James Francis Cooke: 6.376773e-05\n",
      "The Legend of Sleepy Hollow - Washington Irving: 6.051303e-05\n",
      "The Odyssey - Homer: 5.753154e-05\n",
      "Twenty Thousand Leagues under the Sea - Jules Verne: 5.694209e-05\n",
      "The Three Musketeers - Alexandre Dumas and Auguste Maquet: 5.604292e-05\n",
      "Concerning the Spiritual in Art - Wassily Kandinsky: 5.597718e-05\n",
      "Crime and Punishment - Fyodor Dostoyevsky: 5.361711e-05\n",
      "Resonance in Singing and Speaking - Thomas Fillebrown: 5.143421e-05\n",
      "The Prince - Niccolò Machiavelli: 5.116173e-05\n",
      "Divine Comedy - Dante Alighieri: 5.027897e-05\n",
      "Getting Acquainted with the Trees - J Horace McFarland: 4.930764e-05\n",
      "Relativity_The Special and General Theory - Albert Einstein: 4.838410e-05\n",
      "The Life of the Bee - Maurice Maeterlinck: 4.831727e-05\n",
      "A Handbook of Laboratory Glass-Blowing - Bernard D Bolas: 4.417740e-05\n",
      "Two Years Before the Mast - Richard Henry Dana: 4.322219e-05\n",
      "The Aeneid - Virgil: 4.318570e-05\n",
      "The Poetry of Architecture - John Ruskin: 4.308424e-05\n",
      "David Copperfield - Charles Dickens : 4.259720e-05\n",
      "The Glow-Worm and Other Beetles - Jean-Henri Fabre: 4.120467e-05\n",
      "Notre-Dame de Paris - Victor Hugo: 3.806009e-05\n",
      "Life of Chopin - Franz Liszt: 3.792032e-05\n",
      "Butterflies Worth Knowing - Clarence Moores Weed: 3.634129e-05\n",
      "On the Origin of Clockwork_Perpetual Motion Devices_and the Compass - Derek J de Solla Price: 3.450631e-05\n",
      "Moby Dick - Herman Melville: 3.014917e-05\n",
      "The Fern Lover's Companion - George Henry Tilton: 2.533548e-05\n",
      "Encyclopedia of Needlework - Thérèse de Dillmont: 1.749892e-05\n",
      "Handbook of Wool Knitting and Crochet - Anonymous: 0.000000e+00\n",
      "Studies of Trees - Jacob Joshua Levison: 0.000000e+00\n",
      "The Mechanical Properties of Wood - Samuel J Record: 0.000000e+00\n",
      "Wild Flowers Worth Knowing - Neltje Blanchan: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/ProjectGutenberg')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "    \n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "        \n",
    "# Run any query you want!\n",
    "run_query(\"My very own query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, you can run the cell below to see the titles of all the documents in our dataset. As sanity checks, you can try tailoring your queries in `run_query` to output certain titles and/or checking what IR system outputs against the list of titles to see if the results make sense (i.e. the book _Great Pianists on Piano Playing_ should probably be among the top results if the query is \"pianists play piano\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A Handbook of Laboratory Glass-Blowing - Bernard D Bolas\n",
      "2. A Journey to the Centre of the Earth - Jules Verne\n",
      "3. Anna Karenina - Leo Tolstoy\n",
      "4. Antarctic Penguins-A Study of Their Social Habits - G Murray Levick\n",
      "5. Anthem - Ayn Rand\n",
      "6. Around the World in Eighty Days - Jules Verne\n",
      "7. Autobiography - John Stuart Mill\n",
      "8. Butterflies Worth Knowing - Clarence Moores Weed\n",
      "9. Concerning the Spiritual in Art - Wassily Kandinsky\n",
      "10. Crime and Punishment - Fyodor Dostoyevsky\n",
      "11. David Copperfield - Charles Dickens \n",
      "12. Divine Comedy - Dante Alighieri\n",
      "13. Don Quixote - Miguel de Cervantes Saavedra\n",
      "14. Dracula - Bram Stoker\n",
      "15. Encyclopedia of Needlework - Thérèse de Dillmont\n",
      "16. Frankenstein or the Modern Prometheus - Mary Wollstonecraft Shelley\n",
      "17. Getting Acquainted with the Trees - J Horace McFarland\n",
      "18. Great Pianists on Piano Playing - James Francis Cooke\n",
      "19. Handbook of Wool Knitting and Crochet - Anonymous\n",
      "20. Hippolytus and the Bacchae - Euripides\n",
      "21. Les Misérables - Victor Hugo\n",
      "22. Life Is a Dream - Pedro Calderón de la Barca\n",
      "23. Life of Chopin - Franz Liszt\n",
      "24. Macbeth - William Shakespeare\n",
      "25. Metamorphosis - Franz Kafka\n",
      "26. Moby Dick - Herman Melville\n",
      "27. Notre-Dame de Paris - Victor Hugo\n",
      "28. On the Origin of Clockwork_Perpetual Motion Devices_and the Compass - Derek J de Solla Price\n",
      "29. Our Vanishing Wild Life_Its Extermination and Preservation - William T Hornaday\n",
      "30. Pride and Prejudice - Jane Austen\n",
      "31. Relativity_The Special and General Theory - Albert Einstein\n",
      "32. Resonance in Singing and Speaking - Thomas Fillebrown\n",
      "33. Studies of Trees - Jacob Joshua Levison\n",
      "34. The Adventures of Sherlock Holmes - Arthur Conan Doyle\n",
      "35. The Aeneid - Virgil\n",
      "36. The Chemical History of a Candle - Michael Faraday\n",
      "37. The Count of Monte Cristo - Alexandre Dumas and Auguste Maquet\n",
      "38. The Fables of Aesop - Aesop\n",
      "39. The Fern Lover's Companion - George Henry Tilton\n",
      "40. The Glow-Worm and Other Beetles - Jean-Henri Fabre\n",
      "41. The Iliad - Homer\n",
      "42. The Legend of Sleepy Hollow - Washington Irving\n",
      "43. The Life of the Bee - Maurice Maeterlinck\n",
      "44. The Mechanical Properties of Wood - Samuel J Record\n",
      "45. The Odyssey - Homer\n",
      "46. The Oedipus Trilogy - Sophocles\n",
      "47. The Phantom of the Opera - Gaston Leroux\n",
      "48. The Poetry of Architecture - John Ruskin\n",
      "49. The Prince - Niccolò Machiavelli\n",
      "50. The Scarlet Letter - Nathaniel Hawthorne\n",
      "51. The Sorrows of Young Werther - Johann Wolfgang von Goethe\n",
      "52. The Three Musketeers - Alexandre Dumas and Auguste Maquet\n",
      "53. The Time Machine - H G Wells\n",
      "54. The Training of a Forester - Gifford Pinchot\n",
      "55. The War of the Worlds - H G Wells\n",
      "56. Twenty Thousand Leagues under the Sea - Jules Verne\n",
      "57. Two Years Before the Mast - Richard Henry Dana\n",
      "58. Violin Mastery_Talks with Master Violinists and Teachers - Frederick Herman Martens\n",
      "59. War and Peace - Leo Tolstoy\n",
      "60. Wild Flowers Worth Knowing - Neltje Blanchan\n"
     ]
    }
   ],
   "source": [
    "# Prints full list of book titles in dataset (in alphabetical order)\n",
    "for i in range(len(irsys.titles)):\n",
    "    print(\"%d. %s\" % (i + 1, irsys.titles[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Systems and Search Engines\n",
    "\n",
    "Safiya Umoja Noble’s <a href=\"https://nyupress.org/9781479837243/algorithms-of-oppression/\">Algorithms of Oppression</a> (2018) provides insight into how search engines and information retrieval algorithms can exhibit substantial racist and sexist biases. Noble demonstrates how prejudice against black women is embedded into search engine ranked results. These biases are apparent in both Google search’s autosuggestions and the first page of Google results. In this assignment, we have explored numerous features that are built into information retrieval systems, like Google Search. \n",
    "\n",
    "How could the algorithms you built in this assignment contribute to enforcing real-world biases? \n",
    "\n",
    "How can we reduce data discrimination and algorithmic bias that perpetuate gender and racial inequalities in search results and IR system?\n",
    "\n",
    "Please provide a short response to each of these questions (1-2 paragraphs per question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def algorithmic_bias_in_IR_systems(self):\n",
    "        # TODO: Place your response into the response string below\n",
    "        response = \"One way in which the algorithms we built can enforce real-world biases is in our training data (documents). Specifically, if we have a set of documents that we compare queries to not be equally represntative of everything in the real world, then the same biases will persist. If we choose documents that have abnormally high counts of a specific word than another, then when querying, the tf-idf weights will be extremely unbalanced and biased. The cosine similarity based ranking metric that we built automatically defaults some values to 0 if we encounter an out of vocabulary term. This is not a good practice as it will severly underrepresent minority group terms, etc. One way to reduce data discrimination and algorithmic bias is by creating a more representative training dataset. Ensure that all the training documents are diverse and broad. As an extra layer, include something that weights the tf-idf terms or ranking similarities based on the distribution of our training data. \"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found notebook file, creating submission zip...\n",
      "\tzip warning: name not matched: deps/\n",
      "updating: pa4.ipynb (deflated 75%)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa4.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa4.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa4.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa4.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA4 IR assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs124] *",
   "language": "python",
   "name": "conda-env-cs124-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
